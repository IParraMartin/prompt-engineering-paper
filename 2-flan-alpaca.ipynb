{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "import pandas as pd\n",
    "from lexical_diversity import lex_div as ld\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'declare-lab/flan-alpaca-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.config.output_attentions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_and_scores(prompt, reference):\n",
    "\n",
    "    inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)  # Create attention mask\n",
    "    outputs = model.generate(inputs, attention_mask=attention_mask, max_length=50, num_return_sequences=1, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    input_ids = tokenizer.encode(response, return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        \n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(response, reference)\n",
    "\n",
    "    words = response.split()\n",
    "    mtld_score = ld.mtld(words)\n",
    "\n",
    "    response_length = len(response)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
    "        'Recall': [scores[0]['rouge-1']['r'], scores[0]['rouge-2']['r'], scores[0]['rouge-l']['r']],\n",
    "        'Precision': [scores[0]['rouge-1']['p'], scores[0]['rouge-2']['p'], scores[0]['rouge-l']['p']],\n",
    "        'F1 Score': [scores[0]['rouge-1']['f'], scores[0]['rouge-2']['f'], scores[0]['rouge-l']['f']],\n",
    "    })\n",
    "\n",
    "    print(f'ROUGE scores\\n{df}')\n",
    "\n",
    "    return response, perplexity.item(), scores, mtld_score, response_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"Imagine you are an AI expert and explain how transformer architectures work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = \"Sports, spanning from traditional games like football and tennis to emerging fields like eSports, involve strategic, psychological, nutritional, and physical aspects, and they play a significant role in international unity, technological advancement, scientific understanding, and personal development.\"\n",
    "response, perplexity, rouge_scores, mtld, response_length = get_response_and_scores(new_prompt, reference_text)\n",
    "print('\\nResponse:', response)\n",
    "print('Perplexity:', perplexity)\n",
    "print('MTLD:', mtld)\n",
    "print('Response length:', response_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(prompt, response):\n",
    "    # Encode the prompt and response\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the attention weights from the model\n",
    "    outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "    attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy()  # Taking last layer's attention weights\n",
    "    \n",
    "    # Average attention weights across all heads\n",
    "    attention_weights_avg = np.mean(attention_weights, axis=0)\n",
    "    \n",
    "    # Visualize averaged attention weights\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    cax = ax.matshow(attention_weights_avg, cmap='viridis')\n",
    "    \n",
    "    src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    tgt_tokens = tokenizer.convert_ids_to_tokens(response_ids.squeeze().tolist())\n",
    "    \n",
    "    ax.set_xticklabels([''] + src_tokens, rotation=90, fontsize=6)\n",
    "    ax.set_yticklabels([''] + tgt_tokens, fontsize=6)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.title(f'Averaged Attention Map Flan-Alpaca (Baseline)')\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = response.split(\".\")[0] + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention(new_prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_per_head(prompt, response):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "    attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy()\n",
    "\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    \n",
    "\n",
    "    grid_size = int(math.ceil(math.sqrt(num_heads)))\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    axs = axs.ravel()  \n",
    "\n",
    "    for head in range(num_heads):\n",
    "        cax = axs[head].matshow(attention_weights[head], cmap='viridis')\n",
    "        \n",
    "        src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "        tgt_tokens = tokenizer.convert_ids_to_tokens(response_ids.squeeze().tolist())\n",
    "        \n",
    "        axs[head].set_xticks(range(len(src_tokens)))\n",
    "        axs[head].set_yticks(range(len(tgt_tokens)))\n",
    "        axs[head].set_xticklabels(src_tokens, rotation=90, fontsize=6)\n",
    "        axs[head].set_yticklabels(tgt_tokens, fontsize=6)\n",
    "        axs[head].set_title(f'Flan-Alpaca Head {head+1} (RPP)')\n",
    "        fig.colorbar(cax, ax=axs[head], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('attention_heads_rpp_alpaca.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention_per_head(new_prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bar_attention_for_token(prompt, response, token_of_interest):\n",
    "    \n",
    "    # Encode the prompt and response\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the attention weights from the model\n",
    "    outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "    attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy()  # Taking last layer's attention weights\n",
    "    \n",
    "    # Find index of the token of interest\n",
    "    token_index = input_ids[0].tolist().index(tokenizer.encode(token_of_interest, add_special_tokens=False)[0])\n",
    "    \n",
    "    # Extract attention weights for the token of interest\n",
    "    attention_for_token = attention_weights[:, token_index, :]\n",
    "    \n",
    "    # Average attention weights across all heads\n",
    "    attention_weights_avg = np.mean(attention_for_token, axis=0)\n",
    "    \n",
    "    # Tokens in the response\n",
    "    tgt_tokens = tokenizer.convert_ids_to_tokens(response_ids.squeeze().tolist())\n",
    "    \n",
    "    # Plot bar chart\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.bar(tgt_tokens, attention_weights_avg)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.ylabel('Attention Weight', fontsize=12)\n",
    "    plt.title(f'Averaged Attention Weights for Token \"{token_of_interest}\"', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_bar_attention_for_token(new_prompt, response, \"Imagine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁token']\n"
     ]
    }
   ],
   "source": [
    "check_tokens = tokenizer.tokenize(\"token\")\n",
    "print(check_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens_for_display(tokens):\n",
    "    \"\"\"Merge specific tokens for display.\"\"\"\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # If current token is 'x' and the next one is 'y'\n",
    "        if i < len(tokens) - 1 and tokens[i] == '▁architecture' and tokens[i+1] == 's':\n",
    "            merged_tokens.append('architectures')\n",
    "            i += 2 \n",
    "        else:\n",
    "            # remove the \"▁\" for other tokens and append\n",
    "            token = tokens[i]\n",
    "            if token.startswith('▁'):\n",
    "                token = token[1:]\n",
    "            merged_tokens.append(token)\n",
    "            i += 1\n",
    "    return merged_tokens\n",
    "\n",
    "def visualize_encoder_self_attention(prompt):\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create dummy decoder input to satisfy the model's requirements\n",
    "    dummy_decoder_input = torch.zeros_like(input_ids)\n",
    "    \n",
    "    # Get the attention weights from the model\n",
    "    outputs = model(input_ids, decoder_input_ids=dummy_decoder_input, output_attentions=True)\n",
    "    attention_weights = outputs.encoder_attentions[-1][0].squeeze().cpu().detach().numpy()  # Taking last layer's attention weights\n",
    "    \n",
    "    # Average attention weights across all heads\n",
    "    attention_weights_avg = np.mean(attention_weights, axis=0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    cax = ax.matshow(attention_weights_avg, cmap='viridis')\n",
    "    \n",
    "    src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    src_tokens = merge_tokens_for_display(src_tokens)\n",
    "    \n",
    "    ax.set_xticks(range(len(src_tokens)))\n",
    "    ax.set_yticks(range(len(src_tokens)))\n",
    "    ax.set_xticklabels(src_tokens, rotation=90, fontsize=6)\n",
    "    ax.set_yticklabels(src_tokens, fontsize=6)\n",
    "    \n",
    "    plt.title(f'Averaged Encoder Self-Attention Map')\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_encoder_self_attention(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens_for_display(tokens):\n",
    "    \"\"\"Merge specific tokens for display.\"\"\"\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # If current token is '▁transformer' and the next one is 's'\n",
    "        if i < len(tokens) - 1 and tokens[i] == '▁architecture' and tokens[i+1] == 's':\n",
    "            merged_tokens.append('architectures')\n",
    "            i += 2  # skip next token\n",
    "        else:\n",
    "            # Just remove the \"▁\" for other tokens and append\n",
    "            token = tokens[i]\n",
    "            if token.startswith('▁'):\n",
    "                token = token[1:]\n",
    "            merged_tokens.append(token)\n",
    "            i += 1\n",
    "    return merged_tokens\n",
    "\n",
    "def visualize_encoder_self_attention_per_head_modified(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create dummy decoder input to satisfy the model's requirements\n",
    "    dummy_decoder_input = torch.zeros_like(input_ids)\n",
    "    \n",
    "    outputs = model(input_ids, decoder_input_ids=dummy_decoder_input, output_attentions=True)\n",
    "    attention_weights = outputs.encoder_attentions[-1][0].squeeze().cpu().detach().numpy()\n",
    "\n",
    "    src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    src_tokens = merge_tokens_for_display(src_tokens)\n",
    "\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    \n",
    "    grid_size = int(math.ceil(math.sqrt(num_heads)))\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        cax = axs[head].matshow(attention_weights[head, :len(src_tokens), :len(src_tokens)], cmap='viridis')\n",
    "        \n",
    "        axs[head].set_xticks(range(len(src_tokens)))\n",
    "        axs[head].set_yticks(range(len(src_tokens)))\n",
    "        axs[head].set_xticklabels(src_tokens, rotation=90, fontsize=6)\n",
    "        axs[head].set_yticklabels(src_tokens, fontsize=6)\n",
    "        axs[head].set_title(f'Head {head+1} Encoder Self-Attention')\n",
    "        fig.colorbar(cax, ax=axs[head], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('encoder_self_attention_heads_rp_modified.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_encoder_self_attention_per_head_modified(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(p):\n",
    "    p = p[p > 0]  # Filter zero probabilities to avoid log(0)\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "def calculate_cosine_similarity(A, B):\n",
    "    A_flat = A.flatten()\n",
    "    B_flat = B.flatten()\n",
    "    return 1 - cosine(A_flat, B_flat)\n",
    "\n",
    "input_ids = tokenizer.encode(new_prompt, return_tensors=\"pt\")\n",
    "response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy() \n",
    "#[-1] index used to select the attention weights from the last decoder layer\n",
    "#[0] index used to select the attention weights of all heads from that layer\n",
    "normalized_attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate entropy for each token's attention distribution\n",
    "entropies = [calculate_entropy(row) for row in normalized_attention_weights]\n",
    "print(f\"Entropies: {entropies}\")\n",
    "\n",
    "# Example: Calculate cosine similarity between attention distributions of the first and second tokens\n",
    "similarity = calculate_cosine_similarity(normalized_attention_weights[0], normalized_attention_weights[1])\n",
    "print(f\"Cosine Similarity between Token 1 and Token 2: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_rpp = [39.03116, 23.336357, 29.521084, 36.68857, 36.894615, 41.18858, 27.100996, 49.97698, \n",
    "                 45.441147, 49.84302, 54.815666, 33.07863, 33.188187, 28.172062, 49.740562, 42.723232]\n",
    "\n",
    "\n",
    "entropies_sip = [30.80757, 30.747223, 34.910835, 33.746796, 30.191683, 42.594837, 23.846157, 46.296013, \n",
    "                 44.807983, 50.556427, 54.675625, 33.160202, 32.391663, 29.775688, 43.250504, 47.027374]\n",
    "\n",
    "def get_entropy_difference(rp_entropy, si_entropy):\n",
    "    return [i - j for i, j in zip(rp_entropy, si_entropy)]\n",
    "\n",
    "difs = get_entropy_difference(entropies, entropies_sip)\n",
    "print(difs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "plt.fill_between(range(len(entropies_rpp)), entropies_rpp, entropies_sip, color='gray', alpha=0.2)\n",
    "\n",
    "plt.plot(entropies_rpp, marker='o', linestyle='-', color='red', label='Role-play', linewidth=1)\n",
    "plt.plot(entropies_sip, marker='x', linestyle='-', color='blue', label='Standard Instruction', linewidth=1)\n",
    "plt.axvspan(0, 2, color='yellow', alpha=0.2, label='Region 1')\n",
    "plt.axvspan(7, 9, color='orange', alpha=0.2, label='Region 2')\n",
    "\n",
    "plt.xlabel('Attention Heads')\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('SIP-RPP Entropy Values Comparison Across Attention Heads (Last Decoder Layer)', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.2)\n",
    "plt.legend(fontsize=12, loc='upper left')\n",
    "plt.xticks(ticks=range(len(entropies)), labels=[f\"Head {i+1}\" for i in range(len(entropies))], rotation=45)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('entropy_values_comparison.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
