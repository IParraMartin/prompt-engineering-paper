{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "import pandas as pd\n",
    "from lexical_diversity import lex_div as ld\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'declare-lab/flan-alpaca-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.config.output_attentions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_and_scores(prompt, reference):\n",
    "\n",
    "    inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)  # Create attention mask\n",
    "    outputs = model.generate(inputs, attention_mask=attention_mask, max_length=50, num_return_sequences=1, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    input_ids = tokenizer.encode(response, return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        \n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(response, reference)\n",
    "\n",
    "    words = response.split()\n",
    "    mtld_score = ld.mtld(words)\n",
    "\n",
    "    response_length = len(response)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
    "        'Recall': [scores[0]['rouge-1']['r'], scores[0]['rouge-2']['r'], scores[0]['rouge-l']['r']],\n",
    "        'Precision': [scores[0]['rouge-1']['p'], scores[0]['rouge-2']['p'], scores[0]['rouge-l']['p']],\n",
    "        'F1 Score': [scores[0]['rouge-1']['f'], scores[0]['rouge-2']['f'], scores[0]['rouge-l']['f']],\n",
    "    })\n",
    "\n",
    "    print(f'ROUGE scores\\n{df}')\n",
    "\n",
    "    return response, perplexity.item(), scores, mtld_score, response_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"Imagine you are an artificial intelligence expert and explain how transformer architectures work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = \"Sports, spanning from traditional games like football and tennis to emerging fields like eSports, involve strategic, psychological, nutritional, and physical aspects, and they play a significant role in international unity, technological advancement, scientific understanding, and personal development.\"\n",
    "response, perplexity, rouge_scores, mtld, response_length = get_response_and_scores(new_prompt, reference_text)\n",
    "print('\\nResponse:', response)\n",
    "print('Perplexity:', perplexity)\n",
    "print('MTLD:', mtld)\n",
    "print('Response length:', response_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(prompt, response):\n",
    "    # Encode the prompt and response\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the attention weights from the model\n",
    "    outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "    attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy()  # Taking last layer's attention weights\n",
    "    \n",
    "    # Average attention weights across all heads\n",
    "    attention_weights_avg = np.mean(attention_weights, axis=0)\n",
    "    \n",
    "    # Visualize averaged attention weights\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    cax = ax.matshow(attention_weights_avg, cmap='viridis')\n",
    "    \n",
    "    src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    tgt_tokens = tokenizer.convert_ids_to_tokens(response_ids.squeeze().tolist())\n",
    "    \n",
    "    ax.set_xticklabels([''] + src_tokens, rotation=90, fontsize=6)\n",
    "    ax.set_yticklabels([''] + tgt_tokens, fontsize=6)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.title(f'Averaged Attention Map Flan-Alpaca (Baseline)')\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = response.split(\".\")[0] + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention(new_prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_per_head(prompt, response):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "    attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy()\n",
    "\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    \n",
    "\n",
    "    grid_size = int(math.ceil(math.sqrt(num_heads)))\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    axs = axs.ravel()  \n",
    "\n",
    "    for head in range(num_heads):\n",
    "        cax = axs[head].matshow(attention_weights[head], cmap='viridis')\n",
    "        \n",
    "        src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "        tgt_tokens = tokenizer.convert_ids_to_tokens(response_ids.squeeze().tolist())\n",
    "        \n",
    "        axs[head].set_xticks(range(len(src_tokens)))\n",
    "        axs[head].set_yticks(range(len(tgt_tokens)))\n",
    "        axs[head].set_xticklabels(src_tokens, rotation=90, fontsize=6)\n",
    "        axs[head].set_yticklabels(tgt_tokens, fontsize=6)\n",
    "        axs[head].set_title(f'Flan-Alpaca Head {head+1} (RPP)')\n",
    "        fig.colorbar(cax, ax=axs[head], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('attention_heads_rpp_alpaca.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention_per_head(new_prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bar_attention_for_token(prompt, response, token_of_interest):\n",
    "    \n",
    "    # Encode the prompt and response\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the attention weights from the model\n",
    "    outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "    attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy()  # Taking last layer's attention weights\n",
    "    \n",
    "    # Find index of the token of interest\n",
    "    token_index = input_ids[0].tolist().index(tokenizer.encode(token_of_interest, add_special_tokens=False)[0])\n",
    "    \n",
    "    # Extract attention weights for the token of interest\n",
    "    attention_for_token = attention_weights[:, token_index, :]\n",
    "    \n",
    "    # Average attention weights across all heads\n",
    "    attention_weights_avg = np.mean(attention_for_token, axis=0)\n",
    "    \n",
    "    # Tokens in the response\n",
    "    tgt_tokens = tokenizer.convert_ids_to_tokens(response_ids.squeeze().tolist())\n",
    "    \n",
    "    # Plot bar chart\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.bar(tgt_tokens, attention_weights_avg)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.ylabel('Attention Weight', fontsize=12)\n",
    "    plt.title(f'Averaged Attention Weights for Token \"{token_of_interest}\"', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_bar_attention_for_token(new_prompt, response, \"Imagine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_encoder_self_attention(prompt):\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create dummy decoder input to satisfy the model's requirements\n",
    "    dummy_decoder_input = torch.zeros_like(input_ids)\n",
    "    \n",
    "    # Get the attention weights from the model\n",
    "    outputs = model(input_ids, decoder_input_ids=dummy_decoder_input, output_attentions=True)\n",
    "    attention_weights = outputs.encoder_attentions[-1][0].squeeze().cpu().detach().numpy()  # Taking last layer's attention weights\n",
    "    \n",
    "    # Average attention weights across all heads\n",
    "    attention_weights_avg = np.mean(attention_weights, axis=0)\n",
    "    \n",
    "    # Visualize averaged attention weights\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    cax = ax.matshow(attention_weights_avg, cmap='viridis')\n",
    "    \n",
    "    src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    \n",
    "    ax.set_xticks(range(len(src_tokens)))\n",
    "    ax.set_yticks(range(len(src_tokens)))\n",
    "    ax.set_xticklabels(src_tokens, rotation=90, fontsize=6)\n",
    "    ax.set_yticklabels(src_tokens, fontsize=6)\n",
    "    \n",
    "    plt.title(f'Averaged Encoder Self-Attention Map')\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_encoder_self_attention(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def visualize_encoder_self_attention_per_head(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create dummy decoder input to satisfy the model's requirements\n",
    "    dummy_decoder_input = torch.zeros_like(input_ids)\n",
    "    \n",
    "    outputs = model(input_ids, decoder_input_ids=dummy_decoder_input, output_attentions=True)\n",
    "    attention_weights = outputs.encoder_attentions[-1][0].squeeze().cpu().detach().numpy()\n",
    "\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    \n",
    "    grid_size = int(math.ceil(math.sqrt(num_heads)))\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        cax = axs[head].matshow(attention_weights[head], cmap='viridis')\n",
    "        \n",
    "        axs[head].set_xticks(range(len(src_tokens)))\n",
    "        axs[head].set_yticks(range(len(src_tokens)))\n",
    "        axs[head].set_xticklabels(src_tokens, rotation=90, fontsize=6)\n",
    "        axs[head].set_yticklabels(src_tokens, fontsize=6)\n",
    "        axs[head].set_title(f'Head {head+1} Encoder Self-Attention')\n",
    "        fig.colorbar(cax, ax=axs[head], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('encoder_self_attention_heads.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_encoder_self_attention_per_head(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(p):\n",
    "    p = p[p > 0]  # Filter zero probabilities to avoid log(0)\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "def calculate_cosine_similarity(A, B):\n",
    "    A_flat = A.flatten()\n",
    "    B_flat = B.flatten()\n",
    "    return 1 - cosine(A_flat, B_flat)\n",
    "\n",
    "input_ids = tokenizer.encode(new_prompt, return_tensors=\"pt\")\n",
    "response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy()\n",
    "normalized_attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate entropy for each token's attention distribution\n",
    "entropies = [calculate_entropy(row) for row in normalized_attention_weights]\n",
    "print(f\"Entropies: {entropies}\")\n",
    "\n",
    "# Example: Calculate cosine similarity between attention distributions of the first and second tokens\n",
    "similarity = calculate_cosine_similarity(normalized_attention_weights[0], normalized_attention_weights[1])\n",
    "print(f\"Cosine Similarity between Token 1 and Token 2: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
