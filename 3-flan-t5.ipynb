{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigoparra/opt/anaconda3/envs/TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-24 00:44:30.709931: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "import pandas as pd\n",
    "from lexical_diversity import lex_div as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_and_scores(prompt, reference):\n",
    "    # Encode & Decode\n",
    "    inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)  # Create attention mask\n",
    "    outputs = model.generate(inputs, attention_mask=attention_mask, max_length=50, num_return_sequences=1, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    input_ids = tokenizer.encode(response, return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)  # Create attention mask for the response\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        \n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(response, reference)\n",
    "\n",
    "    words = response.split()\n",
    "    mtld_score = ld.mtld(words)\n",
    "\n",
    "    response_length = len(response)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
    "        'Recall': [scores[0]['rouge-1']['r'], scores[0]['rouge-2']['r'], scores[0]['rouge-l']['r']],\n",
    "        'Precision': [scores[0]['rouge-1']['p'], scores[0]['rouge-2']['p'], scores[0]['rouge-l']['p']],\n",
    "        'F1 Score': [scores[0]['rouge-1']['f'], scores[0]['rouge-2']['f'], scores[0]['rouge-l']['f']],\n",
    "    })\n",
    "\n",
    "    print(f'ROUGE scores\\n{df}')\n",
    "\n",
    "    return response, perplexity.item(), scores, mtld_score, response_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores\n",
      "    Metric    Recall  Precision  F1 Score\n",
      "0  ROUGE-1  0.083333   0.333333  0.133333\n",
      "1  ROUGE-2  0.000000   0.000000  0.000000\n",
      "2  ROUGE-L  0.083333   0.333333  0.133333\n",
      "\n",
      "Response: <pad> a new kind of computer</s>\n",
      "Perplexity: 78.39458465576172\n"
     ]
    }
   ],
   "source": [
    "new_prompt = 'Describe the influence of transformer architecture in AI'\n",
    "reference_text = 'Artificial Intelligence, with its ability to analyze massive data sets and learn from patterns, has revolutionized industries like healthcare, finance, and transportation, by driving efficiencies and enabling new services, while also raising ethical considerations around privacy and job displacement.'\n",
    "\n",
    "response, perplexity, rouge_scores, mtld, response_length = get_response_and_scores(new_prompt, reference_text)\n",
    "print('\\nResponse:', response)\n",
    "print('Perplexity:', perplexity)\n",
    "print('MTLD:', mtld)\n",
    "print('Response length:', response_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
