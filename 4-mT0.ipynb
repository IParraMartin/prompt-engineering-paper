{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "import pandas as pd\n",
    "from lexical_diversity import lex_div as ld\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bigscience/mt0-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_and_scores(prompt, reference):\n",
    "    # Encode & Decode\n",
    "    inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)  # Create attention mask\n",
    "    outputs = model.generate(inputs, attention_mask=attention_mask, max_length=50, num_return_sequences=1, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    input_ids = tokenizer.encode(response, return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        \n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(response, reference)\n",
    "\n",
    "    words = response.split()\n",
    "    mtld_score = ld.mtld(words)\n",
    "\n",
    "    response_length = len(response)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
    "        'Recall': [scores[0]['rouge-1']['r'], scores[0]['rouge-2']['r'], scores[0]['rouge-l']['r']],\n",
    "        'Precision': [scores[0]['rouge-1']['p'], scores[0]['rouge-2']['p'], scores[0]['rouge-l']['p']],\n",
    "        'F1 Score': [scores[0]['rouge-1']['f'], scores[0]['rouge-2']['f'], scores[0]['rouge-l']['f']],\n",
    "    })\n",
    "\n",
    "    print(f'ROUGE scores\\n{df}')\n",
    "\n",
    "    return response, perplexity.item(), scores, mtld_score, response_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = 'Describe the influence of transformer architecture in AI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = 'Artificial Intelligence, with its ability to analyze massive data sets and learn from patterns, has revolutionized industries like healthcare, finance, and transportation, by driving efficiencies and enabling new services, while also raising ethical considerations around privacy and job displacement.'\n",
    "response, perplexity, rouge_scores, mtld, response_length = get_response_and_scores(new_prompt, reference_text)\n",
    "print('\\nResponse:', response)\n",
    "print('Perplexity:', perplexity)\n",
    "print('MTLD:', mtld)\n",
    "print('Response length:', response_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.output_attentions = True\n",
    "\n",
    "def visualize_attention(prompt, response):\n",
    "    # Encode the prompt and response\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the attention weights from the model\n",
    "    outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "    attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy()  # Taking last layer's attention weights\n",
    "    \n",
    "    # Average attention weights across all heads\n",
    "    attention_weights_avg = np.mean(attention_weights, axis=0)\n",
    "    \n",
    "    # Visualize averaged attention weights\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    cax = ax.matshow(attention_weights_avg, cmap='viridis')\n",
    "    \n",
    "    src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    tgt_tokens = tokenizer.convert_ids_to_tokens(response_ids.squeeze().tolist())\n",
    "    \n",
    "    ax.set_xticklabels([''] + src_tokens, rotation=90, fontsize=6)\n",
    "    ax.set_yticklabels([''] + tgt_tokens, fontsize=6)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.colorbar(cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention(new_prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_per_head(prompt, response):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(input_ids, decoder_input_ids=response_ids, output_attentions=True)\n",
    "    attention_weights = outputs.decoder_attentions[-1][0].squeeze().cpu().detach().numpy()\n",
    "\n",
    "    num_heads = attention_weights.shape[0]\n",
    "    \n",
    "    # Determine the grid size: square root of number of heads\n",
    "    grid_size = int(math.ceil(math.sqrt(num_heads)))\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    axs = axs.ravel()  # Flatten axs to easily loop through\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        cax = axs[head].matshow(attention_weights[head], cmap='viridis')\n",
    "        \n",
    "        src_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "        tgt_tokens = tokenizer.convert_ids_to_tokens(response_ids.squeeze().tolist())\n",
    "        \n",
    "        axs[head].set_xticks(range(len(src_tokens)))\n",
    "        axs[head].set_yticks(range(len(tgt_tokens)))\n",
    "        axs[head].set_xticklabels(src_tokens, rotation=90, fontsize=6)\n",
    "        axs[head].set_yticklabels(tgt_tokens, fontsize=6)\n",
    "        axs[head].set_title(f'Head {head+1} (RP)')\n",
    "        fig.colorbar(cax, ax=axs[head], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('attention_heads_rp_mt0.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention_per_head(new_prompt, response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
